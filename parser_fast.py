"""
FAST Resume Parser - Uses embeddings + regex instead of LLM parsing
This is 10-20x faster than the LLM-based approach!
"""

import os
import json
import fitz  # PyMuPDF
import pandas as pd
import numpy as np
import re
import time
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Optional
from openai import AzureOpenAI
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed


# ==============================
# CONFIGURATION
# ==============================

load_dotenv()

AZURE_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_EMBED_MODEL = os.getenv("AZURE_OPENAI_EMBEDDING_MODEL", "text-embedding-3-large")
AZURE_EMBED_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01")

CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)

EMBEDDING_BATCH_SIZE = 5  # Process 5 embeddings in parallel
MAX_WORKERS = 3  # Parallel threads for embedding generation


# ==============================
# DATA STRUCTURE
# ==============================

@dataclass
class ResumeRecord:
    file_name: str
    text: str
    name: Optional[str]
    email: Optional[str]
    phone: Optional[str]
    skills: List[str]
    education: List[str]
    experience: List[str]
    summary: Optional[str]
    embedding: Optional[List[float]]


# ==============================
# FAST EXTRACTION WITH REGEX
# ==============================

def extract_text_from_pdf(pdf_path: Path) -> str:
    """Extract text from PDF."""
    text_parts = []
    with fitz.open(pdf_path) as doc:
        for page in doc:
            page_text = page.get_text("text")
            if page_text:
                text_parts.append(page_text.strip())
    return "\n".join(text_parts)


def extract_email(text: str) -> Optional[str]:
    """Extract email using regex."""
    match = re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
    return match.group(0) if match else None


def extract_phone(text: str) -> Optional[str]:
    """Extract phone number using regex."""
    # Matches formats: (123) 456-7890, 123-456-7890, 1234567890, +1-123-456-7890
    patterns = [
        r'\+?\d{1,3}[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
        r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
        r'\d{10}',
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return None


def extract_name(text: str) -> Optional[str]:
    """Extract name from first few lines (heuristic)."""
    lines = text.split('\n')
    # First non-empty line that's not too long and doesn't contain common keywords
    for line in lines[:10]:
        line = line.strip()
        if (line and 
            len(line) < 50 and 
            not any(kw in line.lower() for kw in [
                'resume', 'cv', 'curriculum', 'email', 'phone', 'address',
                'summary', 'objective', 'experience', 'education', 'skills',
                'http', 'www', '@', 'linkedin', 'github'
            ])):
            # Check if it looks like a name (2-4 words, mostly alphabetic)
            words = line.split()
            if 2 <= len(words) <= 4 and all(w.replace('.', '').isalpha() for w in words):
                return line
    return None


def extract_skills(text: str) -> List[str]:
    """Extract skills using keyword matching."""
    # Common tech skills to look for
    skill_keywords = {
        # Programming languages
        'Python', 'Java', 'JavaScript', 'TypeScript', 'C++', 'C#', 'Ruby', 'Go', 'Rust', 'Swift', 'Kotlin',
        'PHP', 'Scala', 'R', 'MATLAB', 'SQL', 'HTML', 'CSS',
        
        # Frameworks & Libraries
        'React', 'Angular', 'Vue', 'Node.js', 'Django', 'Flask', 'FastAPI', 'Spring', 'Express',
        'TensorFlow', 'PyTorch', 'Keras', 'Scikit-learn', 'Pandas', 'NumPy',
        
        # Databases
        'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQL Server', 'Cassandra', 'DynamoDB',
        
        # Cloud & DevOps
        'AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes', 'Jenkins', 'CI/CD', 'Git', 'GitHub', 'GitLab',
        'Terraform', 'Ansible', 'Linux', 'Unix',
        
        # AI/ML
        'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Neural Networks',
        'AI', 'Data Science', 'Statistics',
        
        # Other
        'REST API', 'GraphQL', 'Microservices', 'Agile', 'Scrum', 'JIRA', 'Confluence',
        'Excel', 'PowerPoint', 'Tableau', 'Power BI', 'SAP', 'Salesforce'
    }
    
    found_skills = []
    text_lower = text.lower()
    
    for skill in skill_keywords:
        # Case-insensitive search with word boundaries
        if re.search(r'\b' + re.escape(skill.lower()) + r'\b', text_lower):
            found_skills.append(skill)
    
    return sorted(list(set(found_skills)))[:20]  # Return top 20 unique skills


def extract_education(text: str) -> List[str]:
    """Extract education degrees."""
    degrees = []
    degree_keywords = [
        r'\bPh\.?D\.?\b', r'\bDoctorate\b',
        r'\bMaster\'?s?\b', r'\bM\.?S\.?\b', r'\bM\.?B\.?A\.?\b', r'\bM\.?Tech\.?\b',
        r'\bBachelor\'?s?\b', r'\bB\.?S\.?\b', r'\bB\.?E\.?\b', r'\bB\.?Tech\.?\b',
        r'\bAssociate\'?s?\b', r'\bA\.?S\.?\b',
        r'\bDiploma\b', r'\bCertificate\b'
    ]
    
    lines = text.split('\n')
    for line in lines:
        for pattern in degree_keywords:
            if re.search(pattern, line, re.IGNORECASE):
                degrees.append(line.strip()[:100])  # Limit length
                break
    
    return list(set(degrees))[:5]  # Return top 5 unique


def extract_experience(text: str) -> List[str]:
    """Extract job titles/companies (basic heuristic)."""
    experience = []
    
    # Look for common job title patterns
    title_patterns = [
        r'\b(Engineer|Developer|Manager|Director|Architect|Analyst|Consultant|Designer|Lead|Senior|Junior|Staff)\b',
        r'\b(CEO|CTO|VP|President|Head of)\b'
    ]
    
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        # Skip very short or very long lines
        if 10 < len(line) < 100:
            for pattern in title_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    # Check if line also contains years (likely experience entry)
                    if re.search(r'\b(20\d{2}|19\d{2}|\d{4})\b', line):
                        experience.append(line[:80])
                        break
    
    return list(set(experience))[:10]  # Return top 10 unique


def extract_summary(text: str) -> Optional[str]:
    """Extract first few sentences as summary."""
    # Get first 300 characters after potential header
    lines = text.split('\n')
    content_start = 0
    
    # Skip header lines (name, contact info)
    for i, line in enumerate(lines[:10]):
        if len(line) > 50 and not any(kw in line.lower() for kw in ['email', 'phone', 'address', 'linkedin']):
            content_start = i
            break
    
    summary_text = ' '.join(lines[content_start:content_start+5])
    # Clean up
    summary_text = re.sub(r'\s+', ' ', summary_text).strip()
    return summary_text[:300] if summary_text else None


def quick_parse_resume(text: str, file_name: str) -> dict:
    """Fast parsing using regex - no LLM needed!"""
    return {
        "file_name": file_name,
        "text": text,
        "name": extract_name(text),
        "email": extract_email(text),
        "phone": extract_phone(text),
        "skills": extract_skills(text),
        "education": extract_education(text),
        "experience": extract_experience(text),
        "summary": extract_summary(text),
        "embedding": None  # Will be computed separately
    }


# ==============================
# FAST EMBEDDING GENERATION
# ==============================

def compute_single_embedding(text: str, client: AzureOpenAI) -> Optional[List[float]]:
    """Compute embedding for a single text."""
    try:
        response = client.embeddings.create(
            model=AZURE_EMBED_MODEL,
            input=text[:8000]  # Limit to 8K chars
        )
        vector = np.array(response.data[0].embedding, dtype=float)
        vector /= np.linalg.norm(vector)  # Normalize
        return vector.tolist()
    except Exception as e:
        print(f"âš ï¸ Embedding error: {e}")
        return None


def compute_embeddings_parallel(texts: List[str]) -> List[Optional[List[float]]]:
    """Compute embeddings in parallel for speed."""
    if not (AZURE_API_KEY and AZURE_ENDPOINT):
        print("âš ï¸ Missing Azure credentials for embeddings")
        return [None] * len(texts)
    
    client = AzureOpenAI(
        api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT,
        api_version=AZURE_EMBED_API_VERSION,
    )
    
    embeddings = [None] * len(texts)
    
    # Process in parallel batches
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_idx = {
            executor.submit(compute_single_embedding, text, client): idx
            for idx, text in enumerate(texts)
        }
        
        completed = 0
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                embeddings[idx] = future.result()
                completed += 1
                if completed % 5 == 0 or completed == len(texts):
                    print(f"  âœ“ Generated {completed}/{len(texts)} embeddings")
            except Exception as e:
                print(f"âš ï¸ Failed to generate embedding {idx}: {e}")
    
    return embeddings


# ==============================
# CACHE UTILITIES
# ==============================

def load_from_cache(file_name: str) -> Optional[dict]:
    cache_path = CACHE_DIR / f"{file_name}.json"
    if cache_path.exists():
        with open(cache_path, "r") as f:
            return json.load(f)
    return None


def save_to_cache(file_name: str, data: dict):
    cache_path = CACHE_DIR / f"{file_name}.json"
    with open(cache_path, "w") as f:
        json.dump(data, f, indent=2)


# ==============================
# FAST PARSER CLASS
# ==============================

class FastResumeParser:
    """
    Fast resume parser that uses:
    - Regex for structured extraction (instant)
    - Parallel embedding generation (3-5x faster)
    - No LLM parsing (10-20x faster overall!)
    """
    
    def __init__(self, compute_embeddings: bool = True):
        self.compute_embeddings = compute_embeddings
    
    def parse_folder(self, folder_path: str, force_reparse: bool = False) -> pd.DataFrame:
        """Parse all PDFs in a folder."""
        records = []
        folder = Path(folder_path)
        pdf_files = list(folder.glob("*.pdf"))
        
        print(f"ðŸ“Š Found {len(pdf_files)} PDF files")
        print()
        
        # Step 1: Extract text and quick parse (very fast!)
        print("âš¡ Step 1: Quick parsing with regex...")
        texts_to_embed = []
        indices_to_embed = []
        
        for pdf in pdf_files:
            cached = load_from_cache(pdf.stem) if not force_reparse else None
            
            if cached and cached.get('embedding') is not None:
                print(f"  âœ… Loaded from cache: {pdf.name}")
                records.append(cached)
                continue
            
            # Extract and quick parse
            text = extract_text_from_pdf(pdf)
            parsed = quick_parse_resume(text, pdf.name)
            records.append(parsed)
            
            # Track which need embeddings
            if self.compute_embeddings:
                texts_to_embed.append(text)
                indices_to_embed.append(len(records) - 1)
            
            print(f"  âš¡ Parsed: {pdf.name} (name: {parsed['name']}, skills: {len(parsed['skills'])})")
        
        print(f"âœ… Quick parsing complete: {len(records)} resumes")
        print()
        
        # Step 2: Generate embeddings in parallel (fast!)
        if self.compute_embeddings and texts_to_embed:
            print(f"ðŸš€ Step 2: Generating {len(texts_to_embed)} embeddings in parallel...")
            embeddings = compute_embeddings_parallel(texts_to_embed)
            
            # Assign embeddings back to records
            for embedding, idx in zip(embeddings, indices_to_embed):
                records[idx]['embedding'] = embedding
            
            print("âœ… Embeddings complete")
            print()
        
        # Step 3: Save to cache
        print("ðŸ’¾ Step 3: Saving to cache...")
        for record in records:
            save_to_cache(Path(record['file_name']).stem, record)
        print("âœ… Cache updated")
        print()
        
        return pd.DataFrame(records)
    
    def parse_files(self, pdf_files: List[Path]) -> pd.DataFrame:
        """Parse specific PDF files (for incremental updates)."""
        records = []
        
        print(f"ðŸ“Š Processing {len(pdf_files)} files")
        print()
        
        # Quick parse
        print("âš¡ Quick parsing with regex...")
        texts_to_embed = []
        indices_to_embed = []
        
        for pdf in pdf_files:
            text = extract_text_from_pdf(pdf)
            parsed = quick_parse_resume(text, pdf.name)
            records.append(parsed)
            
            if self.compute_embeddings:
                texts_to_embed.append(text)
                indices_to_embed.append(len(records) - 1)
            
            print(f"  âš¡ Parsed: {pdf.name}")
        
        # Generate embeddings
        if self.compute_embeddings and texts_to_embed:
            print(f"ðŸš€ Generating embeddings in parallel...")
            embeddings = compute_embeddings_parallel(texts_to_embed)
            
            for embedding, idx in zip(embeddings, indices_to_embed):
                records[idx]['embedding'] = embedding
        
        # Save to cache
        for record in records:
            save_to_cache(Path(record['file_name']).stem, record)
        
        return pd.DataFrame(records)


# ==============================
# USAGE EXAMPLE
# ==============================

if __name__ == "__main__":
    import sys
    
    folder = sys.argv[1] if len(sys.argv) > 1 else "data"
    
    print("âš¡ FAST Resume Parser - No LLM needed!")
    print("=" * 60)
    print()
    
    start_time = time.time()
    
    parser = FastResumeParser(compute_embeddings=True)
    df = parser.parse_folder(folder)
    
    elapsed = time.time() - start_time
    
    print("=" * 60)
    print(f"âœ… Parsed {len(df)} resumes in {elapsed:.1f} seconds")
    print(f"âš¡ Average: {elapsed/len(df):.1f} seconds per resume")
    print()
    print("ðŸ“Š Sample results:")
    print(df[['file_name', 'name', 'email', 'skills']].head())
    print()
    print(f"ðŸ’¾ Results cached in: {CACHE_DIR}/")
    print("=" * 60)
